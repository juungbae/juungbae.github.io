---
layout: post
title:  "📚 CS224n 01 - Word2Vector"
date:   2021-02-07 12:36:00 +0900
categories: study
---


해당 내용은 Stanford 강의인 CS224n의 정리 버전이다

### 의미란 무엇인가?

사전적 의미로는 단어, 문구를 표현하는 아이디어. 혹은 사람이 단어나 표식 등을 이용해 표현하고자 하는 것. 우리가 언어적으로 생각하는 의미는 전자에 가깝다. 단어는 표식이고, 그 표식은 뜻으로 대치될 수 있다. 물론 그 반대도 가능.

![/assets/images/cs224n-1/boat.webp](/assets/images/cs224n-1/boat.webp)

우리는 이 사진을 보면 **배** 라고 말하는데, 이건 국어사전에 의하면 `사람이나 짐 따위를 싣고 물 위로 떠다니도록 나무나 쇠 따위로 만든 물건` 을 의미한다. 근데 놀랍게도 저 설명을 그대로 적어도 사람들은 저 그림을 떠올릴 수 있고, 나아가서 배를 생각해 낼 수 있다. 즉, 배 ↔ `사람이나 짐 따위를 싣고 물 위로 떠다니도록 나무나 쇠 따위로 만든 물건` 이 성립한다.

근데 이걸 컴퓨터로 하려면? 뭐 동의어 반의어 사전을 구성할 수 있지만 (WordNet) 세상에 그러기엔 단어가 너무 많다. 그리고 문맥 상 다르게 해석될 여지도 있다. `얘! 가을배가 맛있단다!` 의 배는 위에서 언급한 배는 절대 아니다. 뿐만 아니라 새로 생성된 단어나 외국 문화권의 용어가 엄청 추가될 수 있기 때문에 코스트가 비싸다. 매번 그걸 어떻게 추가하고 있지?

### 바이너리 리스트로 단어 표기하기

그래서 나온 방식이 `One-hot encoding` 이다. 그냥 간단하게 설명하자면.. 3개의 단어가 있을 때

$$
W_1 = [\ 1,\ 0,\ 0\ ] \newline
W_2 = [\ 0,\ 1,\ 0\ ] \newline
W_3 = [\ 0,\ 0,\ 1\ ] \newline
$$

뭐 이렇게 단어를 표기하는 것이다. 벡터 전체에 1이 하나밖에 없어서 `One-hot Encoding` 이라고 한다. 간단하게 단어를 수치화할 수 있다는 장점이 있지만, 두 단어간의 유사도를 계산할 방법이 없다. 사실 이렇게 일일히 하지 말고, 단어가 단어끼리 알아서 서로간의 유사도를 찾게 하는 방법은 없을까? 

### Distributional semantics

NLP의 핵심 아이디어 중 하나. 특정 단어의 의미는, **같이 자주** 등장하는 단어와 관련이 되어 있다는 아이디어이다. 단, 너무 멀리 있는 단어와 연관성은 적으니 어느정도 거리까지 연관된 단어인지를 정하게 되고, 이를 `window 크기` 라고 한다.  

![/assets/images/cs224n-1/2.png](/assets/images/cs224n-1/2.png)

`banking` 이라는 단어와 연관되어 있는 단어들이, 문장에서 해당 단어 옆에 있는 단어들이라는 것이다. 이걸 이용해서, 단어의 벡터를 만들어 다른 단어와의 유사도를 계산할 수 있도록 만든 방법이 워드 벡터이다. 

### Word2Vec

![/assets/images/cs224n-1/3.png](/assets/images/cs224n-1/3.png)

이때 워드벡터의 likelihood와 cost function은 다음과 같다

$$L(\theta) = \prod_{t=1}^{T}\prod_{-m \le j \le m} P(w_{t+j}|w_t;\theta)$$

$$J(\theta) = -\frac{1}{T}\sum_{t=1}^{T}\sum_{-m\le j\le m} \log P(w_{t+j}|w_t;\theta)$$

이 때, 특정 단어 $w$에 대해 Word2Vec은 두가지 벡터를 갖는다.

- $v_w$ 는 해당 단어가 중심에 있을 때
- $u_w$는 해당 단어가 컨텍스트에 있을 때

이 때 컨텍스트 단어 o와 중심 단어 c에 대해

$$P(o|c) = \frac{\exp(u_o^Tv_c)}{\sum_{w \in V} \exp(u_w^Tv_c)}$$

이다. ( 형태를 보면 $\mathbb{R} \rarr (0, 1)^n$  인 Softmax function과 유사하다 )

### Gradient Descent

![/assets/images/cs224n-1/4.png](/assets/images/cs224n-1/4.png)


결국 우리의 목표는 loss function의 값을 최소화 하는 것이다. 완벽한 해를 찾기가 어렵기 때문에, 대개 다음과 같은 방법을 쓰곤 한다. 변화량을 계산하여 빼는 과정을 반복하면서 최소값을 찾아나가는 과정이다.

$$\theta^{new} = \theta^{old} - \alpha \nabla_{\theta}J(\theta)$$

문제는, 여기 이 $J(\theta)$ 가 말뭉치 전체에 관한 함수이기 때문에, $\nabla_{\theta}J(\theta)$ 함수의 계산 비용은 어마어마하다. 그래서 윈도우를 샘플링해서 계산 속도를 빠르게 하는 방법이 있다. 물론 성능이 좋지는 않다.

```python
while True:
	window = sample_window(corpus)
	theta_grad = evaluate_gradient(J, window, theta)
	theta = theta - alpha * theta_grad
```